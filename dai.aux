\relax 
\citation{lilinhui2010}
\citation{dajunzhang2000}
\citation{lianhongcai1995}
\citation{wang1996broad}
\citation{fangzhouliu2007}
\citation{bengio2013representation}
\citation{shan2016bi}
\@writefile{toc}{\contentsline {section}{\numberline {1} Introduction}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Chinese G2P conversion flow\relax }}{1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:chineseG2P}{{1}{1}}
\citation{sotelo2017char2wav}
\citation{wang2017tacotron}
\citation{li2018close}
\citation{devlin2018bert}
\citation{vaswani2017attention}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Model architecture\relax }}{2}}
\newlabel{fig:model_architecture}{{2}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {2} The proposed approach}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces BERT architecture\relax }}{2}}
\newlabel{fig:bert}{{3}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1} The pre-trained BERT}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2} The NN based classifier}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1} Fully-connected network based classifer}{2}}
\citation{shan2016bi}
\citation{shan2016bi}
\citation{shan2016bi}
\citation{devlin2018bert}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The NN based classifiers, (a) fully-connected network based classifier, (b) LSTM based classifier, (c) Transformer block based classifier \relax }}{3}}
\newlabel{fig:cls}{{4}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2} LSTM based classifer}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3} Transformer block based classifier}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {3} Experiment and analysis}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1} Dataset}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2} Settings of baseline approach}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces LSTM baseline approach for polyphone disambiguation\relax }}{3}}
\newlabel{fig:lstmbaseline}{{5}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3} Settings of the proposed approach}{3}}
\citation{vaswani2017attention}
\citation{kingma2014adam}
\citation{vaswani2017attention}
\citation{shan2016bi}
\bibstyle{IEEEtran}
\bibdata{mybib}
\bibcite{lilinhui2010}{1}
\bibcite{dajunzhang2000}{2}
\bibcite{lianhongcai1995}{3}
\bibcite{wang1996broad}{4}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Experimental results: accuracies of defferent methods, (1) LSTM baseline, (2) BERT + FC, (3) BERT + LSTM, (4) BERT + Transformer block\relax }}{4}}
\newlabel{fig:acc}{{6}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4} Experimental results and analysis}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces PCA embedding of semantic features extracted by BERT corresponding to Chinese character, (a) ZHANG, (b) DI\relax }}{4}}
\newlabel{fig:pca}{{7}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces attention average weight cropped around polyphonic character\relax }}{4}}
\newlabel{fig:attenmap}{{8}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {4} Conclusions}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {5} References}{4}}
\bibcite{fangzhouliu2007}{5}
\bibcite{bengio2013representation}{6}
\bibcite{shan2016bi}{7}
\bibcite{sotelo2017char2wav}{8}
\bibcite{wang2017tacotron}{9}
\bibcite{li2018close}{10}
\bibcite{devlin2018bert}{11}
\bibcite{vaswani2017attention}{12}
\bibcite{li2018analogical}{13}
\bibcite{kingma2014adam}{14}
